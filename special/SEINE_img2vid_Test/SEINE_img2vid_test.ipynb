{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SEINE image2video demo(unofficial)\n",
        "\n",
        "* official repo: https://github.com/Vchitect/SEINE/\n",
        "* official web: https://vchitect.github.io/SEINE-project/\n",
        "\n",
        "![000000](https://raw.githubusercontent.com/tfm1102/ComfyUI-AnimateDiff-Colab/main/special/SEINE_img2vid_Test/sample.gif)"
      ],
      "metadata": {
        "id": "K1yHyAyhBjZK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEREDfn_xg3t"
      },
      "outputs": [],
      "source": [
        "#Install SEINE\n",
        "!git clone https://github.com/Vchitect/SEINE\n",
        "%cd SEINE\n",
        "!pip install -r requirement.txt\n",
        "!pip install av\n",
        "!wget -c https://huggingface.co/xinyuanc91/SEINE/resolve/main/seine.pt -P ./pretrained/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download SD1.4 (SEINE is SD1.4 based)\n",
        "!wget -c https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/main/feature_extractor/preprocessor_config.json -P ./pretrained/stable-diffusion-v1-4/feature_extractor/\n",
        "!wget -c https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/main/safety_checker/config.json -P ./pretrained/stable-diffusion-v1-4/safety_checker/\n",
        "!wget -c https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/main/safety_checker/pytorch_model.bin -P ./pretrained/stable-diffusion-v1-4/safety_checker/\n",
        "!wget -c https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/main/scheduler/scheduler_config.json -P ./pretrained/stable-diffusion-v1-4/scheduler/\n",
        "!wget -c https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/main/text_encoder/config.json -P ./pretrained/stable-diffusion-v1-4/text_encoder/\n",
        "!wget -c https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/main/text_encoder/pytorch_model.bin -P ./pretrained/stable-diffusion-v1-4/text_encoder/\n",
        "!wget -c https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/main/tokenizer/merges.txt -P ./pretrained/stable-diffusion-v1-4/tokenizer/\n",
        "!wget -c https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/main/tokenizer/special_tokens_map.json -P ./pretrained/stable-diffusion-v1-4/tokenizer/\n",
        "!wget -c https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/main/tokenizer/tokenizer_config.json -P ./pretrained/stable-diffusion-v1-4/tokenizer/\n",
        "!wget -c https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/main/tokenizer/vocab.json -P ./pretrained/stable-diffusion-v1-4/tokenizer/\n",
        "!wget -c https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/main/unet/config.json -P ./pretrained/stable-diffusion-v1-4/unet/\n",
        "!wget -c https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/main/unet/diffusion_pytorch_model.bin -P ./pretrained/stable-diffusion-v1-4/unet/\n",
        "!wget -c https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/main/vae/config.json -P ./pretrained/stable-diffusion-v1-4/vae/\n",
        "!wget -c https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/main/vae/diffusion_pytorch_model.bin -P ./pretrained/stable-diffusion-v1-4/vae/"
      ],
      "metadata": {
        "id": "MFgc8grky6is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic image to video generation\n",
        "!python sample_scripts/with_mask_sample.py --config configs/sample_i2v.yaml\n",
        "\n",
        "#The generated video will be saved in ./results/i2v.\n",
        "#You may modify ./configs/sample_i2v.yaml to change the generation conditions. For example:\n",
        "##ckpt: is used to specify a model checkpoint.\n",
        "##text_prompt: is used to describe the content of the video.\n",
        "##input_path: is used to specify the path to the image."
      ],
      "metadata": {
        "id": "ccq4R97m1wuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transition video between 2 images\n",
        "!python sample_scripts/with_mask_sample.py --config configs/sample_transition.yaml"
      ],
      "metadata": {
        "id": "w5fzr64SIK0k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}